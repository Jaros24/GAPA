{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import random\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import itertools\n",
    "import json\n",
    "from scipy.optimize import minimize\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING MODE\n",
      "Using default directory, 4 sims, iteration 0\n"
     ]
    }
   ],
   "source": [
    "testing = False\n",
    "\n",
    "# read in the working directories and iteration number\n",
    "if len(sys.argv) == 4:\n",
    "    # set locations for working files\n",
    "    automation_dir = sys.argv[1]\n",
    "    num_sims = int(sys.argv[2])\n",
    "    iteration = int(sys.argv[3])\n",
    "else:\n",
    "    if testing:\n",
    "        print(\"TESTING MODE\")\n",
    "        automation_dir = '/mnt/analysis/e17023/Adam/GADGET2/'\n",
    "        num_sims = 4\n",
    "        iteration = 0\n",
    "        print(f\"Using default directory, {num_sims} sims, iteration {iteration}\")\n",
    "    else:\n",
    "        print(\"Usage: python tuning.py <automation_dir> <num_sims> <iteration>\")\n",
    "        raise ValueError(\"Incorrect number of arguments passed to tuning.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_files():\n",
    "    param_df = pd.DataFrame(columns=['Sim', 'Status', 'P0', 'E0', 'P1', 'E1', 'N', 'Score'])\n",
    "    tuning_log = {\n",
    "        'VarParameters' : { # initial values of tuning parameters, changed to best fit values after each iteration\n",
    "            'Threshold' : 86,\n",
    "            'EIonize' : 22.3,\n",
    "            'Fano' : 0.24,\n",
    "            'CoefL' : 0.000114,\n",
    "            'CoefT' : 0.00284,\n",
    "            'Gain' : 8000,\n",
    "            'GETGain' : 120,\n",
    "            'PeakingTime' : 1014\n",
    "        },\n",
    "        'FixedParameters' : { # parameters that are fixed for all simulations\n",
    "            'Xb' : 99,\n",
    "            'Yb' : 2,\n",
    "            'Seed' : 2,\n",
    "            'CD' : 1,\n",
    "            'CDH' : 1,\n",
    "            'GasPressure' : 800\n",
    "        },\n",
    "        'N': 100, # number of events per simulation\n",
    "        'BestN' : 5, # number of best simulations to use for next iteration\n",
    "        'VarRange' : 0.1, # maximum range of single step variation\n",
    "        'IntParams' : ['Threshold', 'Gain', 'GETGain', 'PeakingTime'], # parameters that must be integers\n",
    "        'Weights' : {}, # weights for each attribute in the score function\n",
    "        'TuningParticles' : [],\n",
    "        'MaxIterations' : 1000\n",
    "    }\n",
    "    \n",
    "    # save tuning information to json file for future iterations\n",
    "    with open(automation_dir + 'tuning_log.json', 'w') as f:\n",
    "        json.dump(tuning_log, f, indent=4)\n",
    "    \n",
    "    # add blank columns for each parameter and save to csv\n",
    "    for param in tuning_log['VarParameters'].keys():\n",
    "        param_df[param] = np.nan\n",
    "    for param in tuning_log['FixedParameters'].keys():\n",
    "        param_df[param] = np.nan\n",
    "    \n",
    "    param_df.to_csv(automation_dir + 'parameters.csv', index=False)\n",
    "    print('Tuning Files Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Files Initialized\n"
     ]
    }
   ],
   "source": [
    "if iteration == 0:\n",
    "    initialize_files()\n",
    "    print('Confirm all particles to tune against are listed in Analysis/real_dirs.csv')\n",
    "    input()\n",
    "    \n",
    "else:\n",
    "    tuning_log = json.load(open(automation_dir + 'tuning_log.json'))\n",
    "    param_df = pd.read_csv(automation_dir + 'parameters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PROCESSING FUNCTIONS\n",
    "def get_energy(image, scale=3000):\n",
    "    # extract energy bar from image\n",
    "    ebar_bounds = ((5,8),(145,17))\n",
    "    ebar = image[ebar_bounds[0][0]:ebar_bounds[1][0], ebar_bounds[0][1]:ebar_bounds[1][1], :]\n",
    "\n",
    "    ebar_slice = np.array([np.mean(ebar[i,1,:]) for i in range(ebar.shape[0])]) # 1d slice of energy bar\n",
    "    for i in range(ebar_slice.shape[0]):\n",
    "        if ebar_slice[i] != 255:\n",
    "            break\n",
    "    proportion_filled = 1 - (i-1)/ebar_slice.shape[0] # proportion of energy bar filled (0-1)\n",
    "    event_energy = (proportion_filled * scale) # scale to max energy\n",
    "    event_energy += 27.766 # offset to match data\n",
    "    return event_energy\n",
    "def get_track(image):\n",
    "    # extract padplane from image\n",
    "    padplane_bounds = ((3,40),(148,185))\n",
    "    padplane = image[padplane_bounds[0][0]:padplane_bounds[1][0], padplane_bounds[0][1]:padplane_bounds[1][1], :]\n",
    "    \n",
    "    # extract track from padplane\n",
    "    track = padplane[:,:,0].copy() # copy red channel for track\n",
    "    track[track == 255] = 0 # set white to black\n",
    "    track_bounds = np.where(track != 0) # get track bounds\n",
    "    track_bounds = ((min(track_bounds[0]), max(track_bounds[0])+1), (min(track_bounds[1]), max(track_bounds[1])+1))\n",
    "    track = track[track_bounds[0][0]:track_bounds[0][1], track_bounds[1][0]:track_bounds[1][1]] # crop track\n",
    "    track = track[::4,::4] # downsample track to remove grid effect\n",
    "    return track\n",
    "def get_trace(image):\n",
    "    trace_img = image[150:,:,0] # extract trace from image\n",
    "    trace_cumsum = np.cumsum(255-trace_img, axis=0) # cumulative sum of trace\n",
    "    trace = trace_cumsum[-1,:].astype(float) # height of trace at each pixel\n",
    "    \n",
    "    trace_diff = np.abs(np.diff(trace))\n",
    "    edges = np.arange(trace_diff.shape[0])[trace_diff > 100] # find edges of trace\n",
    "    \n",
    "    # crop trace_height to edges\n",
    "    trace = trace[edges[0]+5:edges[-1]-5]\n",
    "    \n",
    "    # set baseline to average of first and last 10 pixels\n",
    "    baseline = np.mean(np.concatenate((trace[:10], trace[-10:])))\n",
    "    trace -= baseline # subtract baseline\n",
    "    trace[trace < 0] = 0 # set negative values to 0\n",
    "    \n",
    "    return trace\n",
    "def analyze_trace(trace):\n",
    "    tsum1 = np.cumsum(trace)\n",
    "    tsum2 = np.cumsum(trace[::-1])[::-1]\n",
    "    # find edges of trace peak\n",
    "    cutoff=np.mean(trace) \n",
    "    ledge=np.arange(tsum1.shape[0])[tsum1 >= cutoff][0]\n",
    "    redge=np.arange(tsum2.shape[0])[tsum2 >= cutoff][-1]\n",
    "    \n",
    "    trace_width = redge - ledge # width of trace peak\n",
    "    \n",
    "    trace = trace[ledge:redge] # crop trace to edges\n",
    "    \n",
    "    trace_max = np.max(trace) # peak height of trace\n",
    "    trace_avg = np.mean(trace) # average height of trace (ignoring baseline)\n",
    "    \n",
    "    # determine number of peaks in trace\n",
    "    trace_diff = np.diff(trace)\n",
    "    trace_diff = np.convolve(trace_diff, np.ones(5), mode='same') # smooth trace_diff with moving average\n",
    "    trace_diff[trace_diff <= 0] = -1 # set negative values to -1\n",
    "    trace_diff[trace_diff > 0] = 1 # set positive values to 1\n",
    "    trace_diff = -1*np.diff(trace_diff) # separate to only look for changes in slope direction\n",
    "    num_peaks = np.sum(trace_diff > 0) # number of peaks in trace\n",
    "    \n",
    "    return trace_width, trace_max, trace_avg, num_peaks\n",
    "def analyze_track(track):\n",
    "    length = (track.shape[0]**2 + track.shape[1]**2)**0.5 # length of track\n",
    "    num_pads = track[track>0].reshape(-1).shape[0] # pads in track\n",
    "    width = num_pads / length # width of track\n",
    "    \n",
    "    # number of pixels in track larger than all surrounding pixels in 3x3 window\n",
    "    num_peaks = np.sum(track[1:-1,1:-1] > np.max(np.array([track[:-2,:-2], track[:-2,1:-1], track[:-2,2:], track[1:-1,:-2], track[1:-1,2:], track[2:,:-2], track[2:,1:-1], track[2:,2:]]), axis=0))\n",
    "    \n",
    "    num_noise = 0\n",
    "    # look for free-standing pads with no neighbors\n",
    "    track = np.pad(track, ((1,1),(1,1)), mode='constant', constant_values=0) # pad track with 0s\n",
    "    for i in range(1,track.shape[0]-1):\n",
    "        for j in range(1,track.shape[1]-1):\n",
    "            if track[i,j] > 0 and np.sum(track[i-1:i+2,j]) == track[i,j] and np.sum(track[i,j-1:j+2]) == track[i,j]:\n",
    "                # not including diagonal neighbors\n",
    "                num_noise += 1\n",
    "    num_peaks -= num_noise # subtract free-standing pads from num_peaks\n",
    "    \n",
    "    # pad energy statistics\n",
    "    track = track[track > 0] # remove 0s\n",
    "    max_pad = np.max(track) # highest measured pad energy\n",
    "    min_pad = np.min(track) # lowest measured pad energy\n",
    "    avg_pad = np.mean(track) # average pad energy    \n",
    "    return length, width, num_pads, num_peaks, max_pad, min_pad, avg_pad, num_noise\n",
    "def get_event_length(length, trace_width):\n",
    "    # weight of trace in length calculation\n",
    "    # obtained by minimizing the standard deviation of the length calculation for events of the same energy\n",
    "    trace_weight = 0.59176\n",
    "    \n",
    "    scale = 1/2.2 # scale factor for length calculation (pads to mm)\n",
    "    overshoot = 0 # overshoot of length calculation (mm)\n",
    "    \n",
    "    return scale*(length**2 + trace_weight*trace_width**2)**0.5 - overshoot\n",
    "\n",
    "def Analyze_Image(file_dir):\n",
    "    img_array = np.array(Image.open(file_dir))[:,:,:3]\n",
    "    event_energy = get_energy(img_array)\n",
    "    track = get_track(img_array)\n",
    "    trace = get_trace(img_array)\n",
    "\n",
    "    # normalize energy\n",
    "    track = event_energy * track / np.sum(track) # assumes all energy is represented in track pixels, bad with high threshold\n",
    "    trace = event_energy * trace / np.sum(trace)\n",
    "\n",
    "    trace_width, trace_max, trace_avg, trace_peaks = analyze_trace(trace)\n",
    "    track_length, track_width, num_pads, track_peaks, max_pad, min_pad, avg_pad, num_noise = analyze_track(track)\n",
    "\n",
    "    event_length = get_event_length(track_length, trace_width)\n",
    "    num_peaks = np.max((trace_peaks, track_peaks))\n",
    "\n",
    "    attributes = {\n",
    "        'Energy' : event_energy,\n",
    "        'Length' : event_length,\n",
    "        'Width' : track_width,\n",
    "        'NumPads' : num_pads,\n",
    "        'NumPeaks' : num_peaks,\n",
    "        'MaxPad' : max_pad,\n",
    "        'MinPad' : min_pad,\n",
    "        'AvgPad' : avg_pad,\n",
    "        'NumNoise' : num_noise,\n",
    "        'TraceWidth' : trace_width,\n",
    "        'TraceMax' : trace_max,\n",
    "        'TraceAvg' : trace_avg,\n",
    "        'TracePeaks' : trace_peaks,\n",
    "        'TrackLength' : track_length,\n",
    "        'TrackPeaks' : track_peaks\n",
    "    }\n",
    "    \n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events(event_list):\n",
    "    for event in event_list:\n",
    "        # events with NumNoise > 0 are messy, remove them\n",
    "        if event['NumNoise'] > 0:\n",
    "            event_list.remove(event)\n",
    "            \n",
    "    return event_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'806p': '/mnt/analysis/e21072/h5test/run_0277/len90_ic600000_pads21_eps5_samps5_poly2/673876CUT_Date_12_20_2023/', '1682p': '/mnt/analysis/e21072/h5test/run_0277/len90_ic600000_pads21_eps5_samps5_poly2/241372CUT_Date_12_20_2023/'}\n",
      "Analyzing target 806p images\n",
      "Analyzing target 1682p images\n"
     ]
    }
   ],
   "source": [
    "if iteration == 0: # analyze target images for scoring reference\n",
    "    tuning_dirs = pd.read_csv(f\"{automation_dir}Analysis/real_dirs.csv\", index_col=0).to_dict()['dir']\n",
    "    tuning_log = json.load(open(automation_dir + 'tuning_log.json', 'r'))\n",
    "    param_df = pd.read_csv(automation_dir + 'parameters.csv')\n",
    "    \n",
    "    tuning_log['TuningParticles'] = list(tuning_dirs.keys()) # list of particles to tune against\n",
    "    \n",
    "    # validate that all tuning particles have matches and images exist\n",
    "    for particle in tuning_log['TuningParticles']:\n",
    "        if not os.path.isdir(tuning_dirs[particle]):\n",
    "            raise ValueError(f\"Directory {tuning_dirs[particle]} not found\")\n",
    "        if len(os.listdir(tuning_dirs[particle])) == 0:\n",
    "            raise ValueError(f\"No images found in {tuning_dirs[particle]}\")\n",
    "    \n",
    "    tuning_log['TargetAttributes'] = {} # initialize target attributes\n",
    "    \n",
    "    for particle in tuning_log['TuningParticles']:\n",
    "        print(f\"Analyzing target {particle} images\")\n",
    "        tuning_log['TargetAttributes'][particle] = {} # attributes for each particle type\n",
    "        event_list = []\n",
    "        for file in os.listdir(tuning_dirs[particle]):\n",
    "            if file.endswith('.png'):\n",
    "                event_list.append(Analyze_Image(tuning_dirs[particle] + file))\n",
    "        \n",
    "        # filter out events with bad attributes\n",
    "        for event in event_list:\n",
    "            event_list = filter_events(event_list)\n",
    "        \n",
    "        # average attributes of all images\n",
    "        for attribute in event_list[0].keys():\n",
    "            tuning_log['TargetAttributes'][particle][attribute] = np.mean([event_list[i][attribute] for i in range(len(event_list))])\n",
    "            tuning_log['Weights'][attribute] = 1 # initialize weights to 1 for all attributes\n",
    "        \n",
    "        # save target attributes to json file\n",
    "        with open(automation_dir + 'tuning_log.json', 'w') as f:\n",
    "            json.dump(tuning_log, f, indent=4)\n",
    "        \n",
    "        # save target attributes to csv file\n",
    "        attribute_df = pd.DataFrame(tuning_log['TargetAttributes']).T\n",
    "        attribute_df.index.name = 'Sim'\n",
    "        attribute_df.to_csv(automation_dir + 'AttributesLog.csv', index=True)\n",
    "    \n",
    "    # set default weights to 0 for noise, peaks, and energy attributes\n",
    "    for attribute in tuning_log['Weights'].keys():\n",
    "        if 'noise' in attribute.lower():\n",
    "            tuning_log['Weights'][attribute] = 0\n",
    "        if 'peaks' in attribute.lower():\n",
    "            tuning_log['Weights'][attribute] = 0\n",
    "        if 'energy' in attribute.lower():\n",
    "            tuning_log['Weights'][attribute] = 0\n",
    "    \n",
    "    with open(automation_dir + 'tuning_log.json', 'w') as f:\n",
    "        json.dump(tuning_log, f, indent=4)\n",
    "    \n",
    "    # prompt user to modify tuning_log.json as needed\n",
    "    print('Tuning Log Initialized')\n",
    "    print('Please modify tuning_log.json as needed, then press enter to continue')\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scoring_Function(attributes, particle, tuning_log):\n",
    "    # calculate score for each attribute\n",
    "    score = 0\n",
    "    for attribute in attributes.keys():\n",
    "        weight = tuning_log['Weights'][attribute]\n",
    "        target = tuning_log['TargetAttributes'][particle][attribute]\n",
    "        # Square deviation of attribute from target value (lower is better)\n",
    "        score += weight * (attributes[attribute] - target)**2\n",
    "        if target != 0:\n",
    "            score *= 1/target # normalize score to target value\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score_Simulations(param_df=param_df, image_dir = automation_dir+'Output/images/', tuning_log=tuning_log, automation_dir=automation_dir):\n",
    "    full_image_list = [i for i in os.listdir(image_dir) if i.endswith('.png')]\n",
    "    \n",
    "    Attribute_df = pd.read_csv(automation_dir + 'AttributesLog.csv')\n",
    "    \n",
    "    for index, row in param_df.iterrows():\n",
    "        if row['Score'] == -1 and row['Status'] == 4: # if a completed simulation has not been scored\n",
    "            sim_name = row['Sim']\n",
    "            sim_image_list = [i for i in full_image_list if i.split('_image_')[0] == sim_name]\n",
    "            \n",
    "            ptype = ''\n",
    "            if row['E0'] > 1:\n",
    "                ptype += f\"{int(row['E0'])}{row['P0']}\"\n",
    "            if row['E1'] > 1:\n",
    "                ptype += f\"{int(row['E1'])}{row['P1']}\"\n",
    "            \n",
    "            sim_events = []\n",
    "            for image in sim_image_list:\n",
    "                sim_events.append(Analyze_Image(image_dir + image))\n",
    "            sim_events = filter_events(sim_events)\n",
    "            \n",
    "            sim_attributes = {}\n",
    "            for attribute in sim_events[0].keys():\n",
    "                sim_attributes[attribute] = np.mean([sim_events[i][attribute] for i in range(len(sim_events))])\n",
    "            \n",
    "            # add row for simulation to attribute log\n",
    "            Attribute_df.loc[len(Attribute_df)] = np.nan # add row to attribute log\n",
    "            Attribute_df.loc[len(Attribute_df)-1, 'Sim'] = sim_name\n",
    "            for attribute in sim_attributes.keys():\n",
    "                Attribute_df.loc[len(Attribute_df)-1, attribute] = sim_attributes[attribute]\n",
    "            \n",
    "            # score the simulation\n",
    "            score = Scoring_Function(sim_attributes, ptype, tuning_log)\n",
    "            \n",
    "            # save score to parameter file\n",
    "            param_df.loc[index, 'Score'] = score\n",
    "        \n",
    "        Attribute_df.to_csv(automation_dir + 'AttributesLog.csv', index=False)\n",
    "        \n",
    "    return param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sim(sim_name, ptype, tuning_log=tuning_log, param_df=param_df):\n",
    "    if sim_name not in param_df['Sim'].values:\n",
    "        E0 = 0\n",
    "        E1 = 0\n",
    "        # NEED TO UPDATE THIS IN FUTURE TO SUPPORT MORE THAN P,A,PA EVENTS\n",
    "        if 'p' in ptype: # proton present\n",
    "            E0 = float(ptype.split('p')[0])\n",
    "        if 'a' in ptype: # alpha present\n",
    "            E1 = float(ptype.split('a')[0].split('p')[-1])\n",
    "            \n",
    "        param_df.loc[len(param_df)] = np.nan # add row to parameter file\n",
    "        \n",
    "        # set parameters to default values\n",
    "        param_df.loc[len(param_df)-1, 'Sim'] = sim_name\n",
    "        param_df.loc[len(param_df)-1, 'Status'] = 0\n",
    "        param_df.loc[len(param_df)-1, 'P0'] = 'p'\n",
    "        param_df.loc[len(param_df)-1, 'E0'] = E0\n",
    "        param_df.loc[len(param_df)-1, 'P1'] = 'a'\n",
    "        param_df.loc[len(param_df)-1, 'E1'] = E1\n",
    "        param_df.loc[len(param_df)-1, 'N'] = tuning_log['N']\n",
    "        param_df.loc[len(param_df)-1, 'Score'] = -1\n",
    "        for param in tuning_log['FixedParameters'].keys():\n",
    "            param_df.loc[len(param_df)-1, param] = tuning_log['FixedParameters'][param]\n",
    "        for param in tuning_log['VarParameters'].keys():\n",
    "            param_df.loc[len(param_df)-1, param] = tuning_log['VarParameters'][param]\n",
    "        \n",
    "        for param in tuning_log['VarParameters'].keys():\n",
    "            param_df.loc[len(param_df)-1, param] *= random.uniform(1-tuning_log['VarRange'], 1+tuning_log['VarRange'])\n",
    "        \n",
    "        # round integer parameters\n",
    "        for param in tuning_log['IntParams']:\n",
    "            if param in param_df.columns:\n",
    "                param_df.loc[len(param_df)-1, param] = int(round(param_df.loc[len(param_df)-1, param]))\n",
    "        \n",
    "    else:\n",
    "        print(f\"Simulation {sim_name} already initialized\")\n",
    "    return param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORE SIMULATIONS\n",
    "with open(automation_dir + 'tuning_log.json', 'r') as f:\n",
    "    tuning_log = json.load(f)\n",
    "param_df = pd.read_csv(automation_dir + 'parameters.csv')\n",
    "\n",
    "# Read in the scores from the previous iterations\n",
    "if iteration > 0:\n",
    "    param_df = Score_Simulations()\n",
    "    param_df.to_csv(automation_dir + 'parameters.csv', index=False) # save scores to parameter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAMING CONVENTION:\n",
    "# T{ptype}{iter}\n",
    "\n",
    "# update best fit parameters\n",
    "if len(param_df[param_df['Score'] > -1]) > tuning_log['BestN'] * len(tuning_log['TuningParticles']):\n",
    "    params = tuning_log['VarParameters'] # copy of variable parameters\n",
    "    for param in params.keys():\n",
    "        params[param] = [] # initialize list for each parameter\n",
    "    \n",
    "    for ptype in tuning_log['TuningParticles']:\n",
    "        ptype_df = param_df[param_df['Sim'].str.startswith(f\"T{ptype}\")]\n",
    "        ptype_df = ptype_df[ptype_df['Score'] > -1]\n",
    "        ptype_df = ptype_df.sort_values(by='Score', ascending=True)\n",
    "        ptype_df = ptype_df.head(tuning_log['BestN'])\n",
    "        \n",
    "        for param in params.keys(): # average best fit parameters\n",
    "            params[param].append(np.mean(ptype_df[param]))\n",
    "    \n",
    "    for param in params.keys(): # update tuning_log with best fit parameters\n",
    "        tuning_log['VarParameters'][param] = np.mean(params[param])\n",
    "    \n",
    "    # save tuning_log to json file\n",
    "    with open(automation_dir + 'tuning_log.json', 'w') as f:\n",
    "        json.dump(tuning_log, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUEUE NEW SIMULATIONS IF NEEDED\n",
    "while len(param_df[param_df['Status'] == 0]) < num_sims and len(param_df) < tuning_log['MaxIterations']:\n",
    "    time.sleep(2) # to avoid duplicate sim names\n",
    "    for ptype in tuning_log['TuningParticles']:\n",
    "        nowtime = int(time.time())\n",
    "        sim_name = f\"T{ptype}{nowtime}\"\n",
    "        param_df = initialize_sim(sim_name, ptype, tuning_log, param_df)\n",
    "    # save parameter file\n",
    "    param_df.to_csv(automation_dir + 'parameters.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
