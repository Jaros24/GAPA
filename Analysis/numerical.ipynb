{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from pca import pca\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm import tqdm\n",
    "import peakutils\n",
    "import os, sys\n",
    "from BaselineRemoval import BaselineRemoval\n",
    "from sklearn.cluster import DBSCAN\n",
    "import scipy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "\tdef __enter__(self):\n",
    "\t\tself._original_stdout = sys.stdout\n",
    "\t\tsys.stdout = open(os.devnull, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "\tsys.stdout.close()\n",
    "\tsys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(xset, yset, zset, eset, threshold):\n",
    "\t\"\"\"\n",
    "\tUses DBSCAN to find and remove outliers in 3D data\n",
    "\t\"\"\"\n",
    "\tdata = np.array([xset.T, yset.T, zset.T]).T\n",
    "\tDBSCAN_cluster = DBSCAN(eps=7, min_samples=10).fit(data)\n",
    "\tout_of_cluster_index = np.where(DBSCAN_cluster.labels_==-1)\n",
    "\tdel data\n",
    "\trev = out_of_cluster_index[0][::-1]\n",
    "\t#if len(out_of_cluster_index[0]) > 0:\n",
    "\tfor i in rev:\n",
    "\t\txset = np.delete(xset, i)\n",
    "\t\tyset = np.delete(yset, i)\n",
    "\t\tzset = np.delete(zset, i)\n",
    "\t\teset = np.delete(eset, i)\n",
    "\t#if len(xset) <= threshold:\n",
    "\t#\tveto = True\n",
    "\t#else:\n",
    "    #veto = False\n",
    "\treturn xset, yset, zset, eset, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veto on Length @ 70 mm\n",
      "Veto on Integrated Charge @ 800k\n"
     ]
    }
   ],
   "source": [
    "print('Veto on Length @ 70 mm')\n",
    "print('Veto on Integrated Charge @ 800k')\n",
    "def track_len(xset, yset, zset):\n",
    "    \"\"\"\n",
    "    Uses PCA to find the length of a track\n",
    "    \"\"\"\n",
    "    veto_on_length = False\n",
    " \n",
    "    # Form data matrix\n",
    "    data = np.concatenate((xset[:, np.newaxis], \n",
    "                           yset[:, np.newaxis], \n",
    "                           zset[:, np.newaxis]), \n",
    "                           axis=1)\n",
    "\n",
    "    # Use PCA to find track length\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit(data)\n",
    "    principalComponents = pca.transform(data)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "    \n",
    "    track_len = 2.35*principalDf.std()[0]\n",
    "    track_width = 2.35*principalDf.std()[1]\n",
    "    track_depth = 2.35*principalDf.std()[2]\n",
    "    #if track_len > 70:\n",
    "    #    veto_on_length = True\n",
    "    \n",
    "    return track_len, veto_on_length, track_width, track_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(h5file, threshold):\n",
    "    \"\"\"\n",
    "    This functions does the following: \n",
    "    - Converts h5 files into ndarrays. \n",
    "    - Removes outliers.\n",
    "    - Calls PCA to return track length.\n",
    "    - Sums mesh signal to return energy.\n",
    "    \"\"\"\n",
    "    # Converts h5 files into ndarrays, and output each event dataset as a separte list\n",
    "    num_events = int(len(list(h5file.keys()))/2) \n",
    "    \n",
    "    len_list = []\n",
    "    width_list = []\n",
    "    depth_list = []\n",
    "    good_events = []\n",
    "    tot_energy = []\n",
    "    tracemax_list = []\n",
    "    tracedev_list = []\n",
    "    pads_list = []\n",
    "    \n",
    "    skipped_events = 0\n",
    "    veto_events = 0\n",
    "    \n",
    "    pbar = tqdm(total=num_events+1)\n",
    "    for i in range(0, num_events):\n",
    "        str_event = f\"Event_[{i}]\"\n",
    "        \n",
    "        # Apply pad threshold\n",
    "        event = np.array(h5file[str_event][:])\n",
    "        if len(event) <= threshold:\n",
    "            skipped_events += 1\n",
    "            pbar.update(n=1)\n",
    "            continue\n",
    "            \n",
    "        # Make copy of datasets\n",
    "        dset_0_copyx = event['x']\n",
    "        dset_0_copyy = event['y'] \n",
    "        dset_0_copyz = event['z'] - min(event['z'])\n",
    "        dset_0_copye = event['A']\n",
    "\n",
    "        \n",
    "        # Apply veto condition\n",
    "        R = 36                           # Radius of the pad plane\n",
    "        r = np.sqrt(dset_0_copyx**2 + dset_0_copyy**2)\n",
    "        statements = np.greater(r, R)    # Check if any point lies outside of R\n",
    "      \n",
    "        if np.any(statements) == True:\n",
    "            veto_events += 1\n",
    "            pbar.update(n=1)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # Call remove_outliers to get dataset w/ outliers removed\n",
    "        dset_0_copyx, dset_0_copyy, dset_0_copyz, dset_0_copye, veto = remove_outliers(dset_0_copyx, dset_0_copyy, dset_0_copyz, dset_0_copye, threshold)\n",
    "        if veto == True:\n",
    "            skipped_events += 1\n",
    "            pbar.update(n=1)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Call track_len() to create lists of all track lengths\n",
    "        length, veto_on_length, width, depth = track_len(dset_0_copyx, dset_0_copyy, dset_0_copyz)\n",
    "        if veto_on_length == True:\n",
    "            veto_events += 1\n",
    "            pbar.update(n=1)\n",
    "            continue\n",
    "\n",
    "        \n",
    "       \tstr_trace = f\"Trace_[{i}]\"\n",
    "        trace = np.array(h5file[str_trace][:])\n",
    "        max_val = np.argmax(trace)\n",
    "        low_bound = max_val - 75\n",
    "        if low_bound < 0:\n",
    "            low_bound = 5\n",
    "        upper_bound = max_val + 75\n",
    "        if upper_bound > 511:\n",
    "            upper_bound = 506\n",
    "        trace = trace[low_bound:upper_bound]\n",
    "\n",
    "        polynomial_degree=2 \n",
    "        baseObj=BaselineRemoval(trace)\n",
    "        trace=baseObj.IModPoly(polynomial_degree)\n",
    "\n",
    "        #if np.sum(trace) > 800000:\n",
    "        #    veto_events += 1\n",
    "        #    pbar.update(n=1)\n",
    "        #    continue\n",
    "\n",
    "        len_list.append(length)\n",
    "        width_list.append(width)\n",
    "        depth_list.append(depth)\n",
    "        tot_energy.append(np.sum(trace))\n",
    "        tracemax_list.append(np.max(trace))\n",
    "        tracedev_list.append(np.std(trace))\n",
    "        pads_list.append(len(trace[trace > 0]))\n",
    "\n",
    "        # Track event number of good events\n",
    "        good_events.append(i)  \n",
    "        pbar.update(n=1)\n",
    "\n",
    "    return (tot_energy, skipped_events, veto_events, good_events, len_list, width_list, depth_list, tracemax_list, tracedev_list, pads_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Adam/OneDrive - Tenundra, Inc/Desktop/Programming/GADGET2/simOutput/test_1000AA.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [00:01<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Adam/OneDrive - Tenundra, Inc/Desktop/Programming/GADGET2/simOutput/test_1200proton500alpha.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [00:01<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Adam/OneDrive - Tenundra, Inc/Desktop/Programming/GADGET2/simOutput/test_500alpha.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [00:01<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Adam/OneDrive - Tenundra, Inc/Desktop/Programming/GADGET2/simOutput/test_800PP.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [00:01<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Adam/OneDrive - Tenundra, Inc/Desktop/Programming/GADGET2/simOutput/test_800proton.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [00:01<00:00, 17.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# assign directory\n",
    "directory = \"C:/Users/Adam/OneDrive - Tenundra, Inc/Desktop/Programming/GADGET2/simOutput/\"\n",
    "\n",
    "output_df = pd.DataFrame(columns=['file','event','length', 'width', 'depth', 'tracesum', 'tracemax', 'tracedev', 'padnum'])\n",
    "\n",
    "file_names = []\n",
    "# iterate over files in directory\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        if f[-3:] == '.h5':\n",
    "            print(f)\n",
    "            file_names.append(filename)\n",
    "            h5f = h5py.File(directory+filename, 'r')\n",
    "            (tot_energy, skipped_events, veto_events, good_events, len_list, width_list, depth_list, tracemax_list, tracedev_list, pads_list) = main(h5file=h5f, threshold=15)\n",
    "            for event in range(len(tot_energy)):\n",
    "                output_df = output_df.append({'file' : filename, \n",
    "                                              'event' : event, \n",
    "                                              'length' : len_list[event], \n",
    "                                              'width' : width_list[event], \n",
    "                                              'depth' : depth_list[event], \n",
    "                                              'tracesum' : tot_energy[event], \n",
    "                                              'tracemax' : tracemax_list[event], \n",
    "                                              'tracedev' : tracedev_list[event], \n",
    "                                              'padnum' : pads_list[event]}\n",
    "                                             ,ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1000AA.h5\n",
      "length:\t 12.854182468417406 \t 1.234292535250228\n",
      "width:\t 11.32933332281868 \t 1.1733539683701781\n",
      "depth:\t 3.489283242051298 \t 0.6246687687152638\n",
      "tracesum:\t 1136218.7227491443 \t 4941.553338063969\n",
      "tracemax:\t 94085.81097318664 \t 7250.701672981606\n",
      "tracedev:\t 21225.829628087704 \t 941.0217809746528\n",
      "padnum:\t 110.65 \t 2.4553914898486258\n",
      "\n",
      "test_1200proton500alpha.h5\n",
      "length:\t 29.33395548862193 \t 0.9531753486521912\n",
      "width:\t 11.04855454553013 \t 1.329693434266354\n",
      "depth:\t 4.207750856062226 \t 0.9331898564105945\n",
      "tracesum:\t 1282507.7930782863 \t 6670.947741358467\n",
      "tracemax:\t 87417.65821503835 \t 17336.78657707007\n",
      "tracedev:\t 21580.062263199015 \t 2228.630223413188\n",
      "padnum:\t 98.76470588235294 \t 10.951765906491438\n",
      "\n",
      "test_500alpha.h5\n",
      "length:\t 10.937878768787058 \t 1.024929658365177\n",
      "width:\t 10.281309210032825 \t 1.0069768707900644\n",
      "depth:\t 3.9772690334546175 \t 0.9173206089857148\n",
      "tracesum:\t 415395.7403382207 \t 2355.097586658481\n",
      "tracemax:\t 34045.76139989777 \t 2679.074130350298\n",
      "tracedev:\t 7717.5834161334105 \t 346.4861678125692\n",
      "padnum:\t 109.8 \t 2.6872016358247004\n",
      "\n",
      "test_800PP.h5\n",
      "length:\t 22.580776021781183 \t 3.7058255059534657\n",
      "width:\t 13.29730840586933 \t 1.5890810188765\n",
      "depth:\t 4.864076180727752 \t 1.0876080776569033\n",
      "tracesum:\t 1210804.1296968772 \t 5162.301078559011\n",
      "tracemax:\t 92541.25984527876 \t 10398.3026875556\n",
      "tracedev:\t 21640.84730859293 \t 1326.9215067225668\n",
      "padnum:\t 106.05 \t 6.762707337029734\n",
      "\n",
      "test_800proton.h5\n",
      "length:\t 16.425994241774262 \t 1.0562350538606722\n",
      "width:\t 10.400016579636649 \t 1.1515532009550393\n",
      "depth:\t 3.737861464009927 \t 0.7546955374446881\n",
      "tracesum:\t 664620.7605580678 \t 3235.9697135718607\n",
      "tracemax:\t 54014.61660540329 \t 4454.138006538228\n",
      "tracedev:\t 12285.850929100898 \t 590.389137242362\n",
      "padnum:\t 109.9 \t 3.6113855046447054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in file_names:\n",
    "    print(file)\n",
    "    print('length:\\t', output_df[output_df['file'] == file]['length'].mean(),'\\t', output_df[output_df['file'] == file]['length'].std())\n",
    "    print('width:\\t', output_df[output_df['file'] == file]['width'].mean(),'\\t', output_df[output_df['file'] == file]['width'].std())\n",
    "    print('depth:\\t', output_df[output_df['file'] == file]['depth'].mean(),'\\t', output_df[output_df['file'] == file]['depth'].std())\n",
    "    print('tracesum:\\t', output_df[output_df['file'] == file]['tracesum'].mean(),'\\t', output_df[output_df['file'] == file]['tracesum'].std())\n",
    "    print('tracemax:\\t', output_df[output_df['file'] == file]['tracemax'].mean(),'\\t', output_df[output_df['file'] == file]['tracemax'].std())\n",
    "    print('tracedev:\\t', output_df[output_df['file'] == file]['tracedev'].mean(),'\\t', output_df[output_df['file'] == file]['tracedev'].std())\n",
    "    print('padnum:\\t', output_df[output_df['file'] == file]['padnum'].mean(),'\\t', output_df[output_df['file'] == file]['padnum'].std())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c295b91a4a4f8e66f37da6a2fbf5c84e6919990d10548059361442497be2c972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
