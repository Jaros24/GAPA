{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "#from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os, sys\n",
    "#from BaselineRemoval import BaselineRemoval\n",
    "import scipy\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "\tdef __enter__(self):\n",
    "\t\tself._original_stdout = sys.stdout\n",
    "\t\tsys.stdout = open(os.devnull, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "\tsys.stdout.close()\n",
    "\tsys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_func(data, n_components):\n",
    "    \n",
    "    # original method\n",
    "    #pca = PCA(n_components=n_components)\n",
    "    #pca.fit(data)\n",
    "    #out = pca.transform(data)\n",
    "\n",
    "    \n",
    "    # fishtank-compatible method\n",
    "    # Subtract the mean from the data to center it\n",
    "    mean = np.mean(data, axis=0)\n",
    "    data_centered = data - mean\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov = np.cov(data_centered, rowvar=False)\n",
    "\n",
    "    # Compute the eigenvectors and eigenvalues of the covariance matrix\n",
    "    eigvals, eigvecs = np.linalg.eig(cov)\n",
    "\n",
    "    # Transform the data into the new coordinate system defined by the eigenvectors\n",
    "    out = np.dot(data_centered, eigvecs)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(X, eps, min_samples):\n",
    "    # Initialize variables\n",
    "    labels = np.zeros(X.shape[0])\n",
    "    cluster = 0\n",
    "\n",
    "    # Compute distances between points\n",
    "    dists = np.sqrt(((X[:, np.newaxis] - X) ** 2).sum(axis=2))\n",
    "\n",
    "    # Iterate over each point\n",
    "    for i in range(X.shape[0]):\n",
    "        # If point already visited, continue\n",
    "        if labels[i] != 0:\n",
    "            continue\n",
    "\n",
    "        # Find neighboring points\n",
    "        neighbors = np.where(dists[i] <= eps)[0]\n",
    "\n",
    "        # If not enough neighboring points, label as noise\n",
    "        if len(neighbors) < min_samples:\n",
    "            labels[i] = -1\n",
    "            continue\n",
    "\n",
    "        # Expand cluster\n",
    "        cluster += 1\n",
    "        labels[i] = cluster\n",
    "\n",
    "        while len(neighbors) > 0:\n",
    "            j = neighbors[0]\n",
    "            if labels[j] == -1:\n",
    "                labels[j] = cluster\n",
    "            elif labels[j] == 0:\n",
    "                labels[j] = cluster\n",
    "                new_neighbors = np.where(dists[j] <= eps)[0]\n",
    "                if len(new_neighbors) >= min_samples:\n",
    "                    neighbors = np.concatenate((neighbors, new_neighbors))\n",
    "            neighbors = neighbors[1:]\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(xset, yset, zset, eset, threshold):\n",
    "\t#Uses DBSCAN to find and remove outliers in 3D data\n",
    "\t# NEEDS ALTERNATE METHOD FOR FISHTANK COMPATIBILITY\n",
    "\tdata = np.array([xset.T, yset.T, zset.T]).T\n",
    "\n",
    "\t# STANDARD METHOD\n",
    "\t#DBSCAN_cluster = DBSCAN(eps=7, min_samples=10).fit(data)\n",
    "\t#out_of_cluster_index = np.where(DBSCAN_cluster.labels_==-1)\n",
    "\n",
    "\t# FISHTANK METHOD\n",
    "\tlabels = dbscan(data, eps=7, min_samples=10)\n",
    "\tout_of_cluster_index = np.where(labels==-1)\n",
    "\n",
    "\tdel data\n",
    "\trev = out_of_cluster_index[0][::-1]\n",
    "\t\n",
    "\tfor i in rev:\n",
    "\t\txset = np.delete(xset, i)\n",
    "\t\tyset = np.delete(yset, i)\n",
    "\t\tzset = np.delete(zset, i)\n",
    "\t\teset = np.delete(eset, i)\n",
    "\tif len(xset) <= threshold:\n",
    "\t\tveto = True\n",
    "\telse:\n",
    "\t\tveto = False\n",
    "\t\n",
    "\t# testing\n",
    "\tveto = False\n",
    "\n",
    "\treturn xset, yset, zset, eset, veto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veto on Length @ 70 mm\n",
      "Veto on Integrated Charge @ 800k\n"
     ]
    }
   ],
   "source": [
    "print('Veto on Length @ 70 mm')\n",
    "print('Veto on Integrated Charge @ 800k')\n",
    "def track_len(xset, yset, zset):\n",
    "    \"\"\"\n",
    "    Uses PCA to find the length of a track\n",
    "    \"\"\"\n",
    "    veto_on_length = False\n",
    " \n",
    "    # Form data matrix\n",
    "    data = np.concatenate((xset[:, np.newaxis], \n",
    "                           yset[:, np.newaxis], \n",
    "                           zset[:, np.newaxis]), \n",
    "                           axis=1)\n",
    "\n",
    "    # Use PCA to find track length\n",
    "    principalComponents = pca_func(data, 3)\n",
    "\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "    \n",
    "    track_len = 2.35*principalDf.std()[0]\n",
    "    track_width = 2.35*principalDf.std()[1]\n",
    "    track_depth = 2.35*principalDf.std()[2]\n",
    "    #if track_len > 70:\n",
    "    #    veto_on_length = True\n",
    "    \n",
    "    return track_len, veto_on_length, track_width, track_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_baseline_removal(data, degree):\n",
    "        # Compute the integrated signal\n",
    "        integral = np.cumsum(data)\n",
    "        # Fit a polynomial to the integrated signal\n",
    "        p = np.polyfit(np.array(range(len(data))), integral, degree)\n",
    "        # Evaluate the polynomial at each point in the signal\n",
    "        baseline = np.polyval(p, np.array(range(len(data))))\n",
    "        # Subtract the baseline from the signal\n",
    "        corrected = data - baseline\n",
    "        # Return the corrected signal\n",
    "        return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(h5file, threshold):\n",
    "    \"\"\"\n",
    "    This functions does the following: \n",
    "    - Converts h5 files into ndarrays. \n",
    "    - Removes outliers.\n",
    "    - Calls PCA to return track length.\n",
    "    - Sums mesh signal to return energy.\n",
    "    \"\"\"\n",
    "    # Converts h5 files into ndarrays, and output each event dataset as a separte list\n",
    "    num_events = int(len(list(h5file.keys()))/2) \n",
    "    \n",
    "    length_list = []\n",
    "    width_list = []\n",
    "    good_events = []\n",
    "    tot_energy = []\n",
    "    tracemax_list = []\n",
    "    tracedev_list = []\n",
    "    padnum_list = []\n",
    "\n",
    "    \n",
    "    skipped_events = 0\n",
    "    veto_events = 0\n",
    "    \n",
    "    #pbar = tqdm(total=num_events+1)\n",
    "    for i in range(0, num_events):\n",
    "        str_event = f\"Event_[{i}]\"\n",
    "        \n",
    "        # Apply pad threshold\n",
    "        event = np.array(h5file[str_event][:])\n",
    "        if len(event) <= threshold:\n",
    "            skipped_events += 1\n",
    "            #pbar.update(n=1)\n",
    "            continue\n",
    "            \n",
    "        # Make copy of datasets\n",
    "        dset_0_copyx = event['x']\n",
    "        dset_0_copyy = event['y'] \n",
    "        dset_0_copyz = event['z'] - min(event['z'])\n",
    "        dset_0_copye = event['A']\n",
    "        \n",
    "        # Apply veto condition\n",
    "        R = 36                           # Radius of the pad plane\n",
    "        r = np.sqrt(dset_0_copyx**2 + dset_0_copyy**2)\n",
    "        statements = np.greater(r, R)    # Check if any point lies outside of R\n",
    "      \n",
    "        if np.any(statements) == True:\n",
    "            veto_events += 1\n",
    "            #pbar.update(n=1)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # Call remove_outliers to get dataset w/ outliers removed\n",
    "        dset_0_copyx, dset_0_copyy, dset_0_copyz, dset_0_copye, veto = remove_outliers(dset_0_copyx, dset_0_copyy, dset_0_copyz, dset_0_copye, threshold)\n",
    "        veto = False\n",
    "        if veto == True:\n",
    "            skipped_events += 1\n",
    "            #pbar.update(n=1)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Call track_len() to create lists of all track lengths\n",
    "        length, veto_on_length, width, depth = track_len(dset_0_copyx, dset_0_copyy, dset_0_copyz)\n",
    "        if veto_on_length == True:\n",
    "            veto_events += 1\n",
    "            #pbar.update(n=1)\n",
    "            continue\n",
    "\n",
    "        \n",
    "       \tstr_trace = f\"Trace_[{i}]\"\n",
    "        trace = np.array(h5file[str_trace][:])\n",
    "        max_val = np.argmax(trace)\n",
    "        low_bound = max_val - 75\n",
    "        if low_bound < 0:\n",
    "            low_bound = 5\n",
    "        upper_bound = max_val + 75\n",
    "        if upper_bound > 511:\n",
    "            upper_bound = 506\n",
    "        trace = trace[low_bound:upper_bound]\n",
    "\n",
    "        # STANDARD METHOD\n",
    "        #baseObj=BaselineRemoval(trace)\n",
    "        #trace=baseObj.IModPoly(polynomial_degree)\n",
    "\n",
    "        # FISHTANK METHOD\n",
    "        # determine the width of the peak in the trace and the location of the peak\n",
    "        peakloc = np.argmax(trace)\n",
    "        \n",
    "        peakwidth1 = 0\n",
    "        peakwidth2 = 0\n",
    "\n",
    "        i = peakloc\n",
    "        while trace[i] > np.min(trace) + np.std(trace):\n",
    "            peakwidth1 += 1\n",
    "            i += 1\n",
    "        i = peakloc\n",
    "        while trace[i] > np.min(trace) + np.std(trace):\n",
    "            peakwidth2 += 1\n",
    "            i -= 1\n",
    "\n",
    "        # calculate the average of the trace outside of the peakwidth window on either side of the peak\n",
    "        baseline = np.mean(np.concatenate((trace[:peakloc-peakwidth2], trace[peakloc+peakwidth1:])))\n",
    "        # subtract the baseline from the trace\n",
    "        trace = trace - baseline\n",
    "\n",
    "        # ADD NOISE TO TRACE\n",
    "        ADD_NOISE = True\n",
    "        # noise of frequency ~0.25 with random phase\n",
    "        if ADD_NOISE:\n",
    "            noise_peak = np.sin(np.linspace(0, 2*np.pi*len(trace)*random.gauss(2.49815e-1, -8.27021e-3), len(trace)) + random.random()*2*np.pi)\n",
    "\n",
    "            noise_background = np.zeros_like(trace)\n",
    "            for freq in np.linspace(0.1, 0.5):\n",
    "                noise_background += np.sin(np.linspace(0, 2*np.pi*len(trace)*freq, len(trace)) + random.random()*2*np.pi) * random.gauss(1, 0.3)\n",
    "            noise_background = noise_background - np.mean(noise_background)\n",
    "            noise_background = noise_background / np.std(noise_background)\n",
    "\n",
    "\n",
    "            noise = 13000*noise_peak + 2000 * noise_background\n",
    "\n",
    "            # normalize noise to std of 1 and average of 0, then multiply by amplitude of real data (402.57 +- 106.40 std)\n",
    "            noise = (noise - np.mean(noise))/np.std(noise) * random.gauss(402.57, 106.40)\n",
    "            # add noise to trace\n",
    "            trace = trace + noise\n",
    "         \n",
    "        # VETO ON TRACE SUM\n",
    "        #if np.sum(trace) > 800000:\n",
    "        #    veto_events += 1\n",
    "        #    pbar.update(n=1)\n",
    "        #    continue\n",
    "\n",
    "        length_list.append(length)\n",
    "        width_list.append(width)\n",
    "        tot_energy.append(np.sum(trace))\n",
    "        padnum_list.append(len(dset_0_copyx))\n",
    "        tracemax_list.append(np.max(trace))\n",
    "        tracedev_list.append(np.std(trace))\n",
    "        \n",
    "\n",
    "        # Track event number of good events\n",
    "        good_events.append(i)  \n",
    "        #pbar.update(n=1)\n",
    "\n",
    "    return (tot_energy, skipped_events, veto_events, good_events, length_list, width_list, tracemax_list, tracedev_list, padnum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/analysis/e17023/Adam/GADGET2/simOutput/273_800p.h5\n"
     ]
    }
   ],
   "source": [
    "# assign directory\n",
    "directory = \"/mnt/analysis/e17023/Adam/GADGET2/simOutput/\"\n",
    "#\"C:\\\\Users\\\\Adam\\\\OneDrive - Tenundra, Inc\\\\Desktop\\\\Programming\\\\GADGET2\\\\simOutput\\\\\"\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame(columns=['file','event','length', 'width', 'tracesum', 'tracemax', 'tracedev', 'padnum'])\n",
    "\n",
    "select = ['273_800p.h5']\n",
    "\n",
    "file_names = ['273_800p.h5']\n",
    "# iterate over files in directory\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file \n",
    "    if os.path.isfile(f) and filename in select:\n",
    "        if f[-3:] == '.h5':\n",
    "            print(f)\n",
    "            file_names.append(filename)\n",
    "            h5f = h5py.File(directory+filename, 'r')\n",
    "            (tot_energy, skipped_events, veto_events, good_events, len_list, width_list, tracemax_list, tracedev_list, pads_list) = main(h5file=h5f, threshold=15)\n",
    "            for event in range(len(tot_energy)):\n",
    "                output_df = output_df.append({'file' : filename, \n",
    "                                              'event' : event, \n",
    "                                              'length' : len_list[event], \n",
    "                                              'width' : width_list[event], \n",
    "                                              'tracesum' : tot_energy[event], \n",
    "                                              'tracemax' : tracemax_list[event], \n",
    "                                              'tracedev' : tracedev_list[event], \n",
    "                                              'padnum' : pads_list[event]}\n",
    "                                             ,ignore_index = True)\n",
    "            h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273_800p.h5\n",
      "length:\t 13.47481712935178 \t 1.1701253051807101\n",
      "width:\t 4.399283145288104 \t 2.10221647449801\n",
      "tracesum:\t 147879.7616811899 \t 3423.7458318129693\n",
      "tracemax:\t 17394.576926101814 \t 3773.054028922742\n",
      "tracedev:\t 3436.78368009452 \t 389.5521991060988\n",
      "padnum:\t 28.71005917159763 \t 7.452086120815371\n",
      "\n",
      "273_800p.h5\n",
      "length:\t 13.47481712935178 \t 1.1701253051807101\n",
      "width:\t 4.399283145288104 \t 2.10221647449801\n",
      "tracesum:\t 147879.7616811899 \t 3423.7458318129693\n",
      "tracemax:\t 17394.576926101814 \t 3773.054028922742\n",
      "tracedev:\t 3436.78368009452 \t 389.5521991060988\n",
      "padnum:\t 28.71005917159763 \t 7.452086120815371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in file_names:\n",
    "    print(file)\n",
    "    print('length:\\t', output_df[output_df['file'] == file]['length'].mean(),'\\t', output_df[output_df['file'] == file]['length'].std())\n",
    "    print('width:\\t', output_df[output_df['file'] == file]['width'].mean(),'\\t', output_df[output_df['file'] == file]['width'].std())\n",
    "    print('tracesum:\\t', output_df[output_df['file'] == file]['tracesum'].mean(),'\\t', output_df[output_df['file'] == file]['tracesum'].std())\n",
    "    print('tracemax:\\t', output_df[output_df['file'] == file]['tracemax'].mean(),'\\t', output_df[output_df['file'] == file]['tracemax'].std())\n",
    "    print('tracedev:\\t', output_df[output_df['file'] == file]['tracedev'].mean(),'\\t', output_df[output_df['file'] == file]['tracedev'].std())\n",
    "    print('padnum:\\t', output_df[output_df['file'] == file]['padnum'].mean(),'\\t', output_df[output_df['file'] == file]['padnum'].std())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('event summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "c295b91a4a4f8e66f37da6a2fbf5c84e6919990d10548059361442497be2c972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
