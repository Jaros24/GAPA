{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import random\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import stats\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING MODE\n"
     ]
    }
   ],
   "source": [
    "testing = False\n",
    "rseed = 0\n",
    "\n",
    "# read in the working directories and iteration number\n",
    "if len(sys.argv) == 4:\n",
    "    # set locations for working files\n",
    "    automation_dir = sys.argv[1]\n",
    "    attpcroot_dir = sys.argv[2]\n",
    "    \n",
    "    iteration = int(sys.argv[3])\n",
    "else:\n",
    "    if testing:\n",
    "        print(\"TESTING MODE\")\n",
    "        automation_dir = '/mnt/analysis/e17023/Adam/GADGET2/'\n",
    "        attpcroot_dir = '/mnt/analysis/e17023/Adam/ATTPCROOTv2/'\n",
    "        iteration = 1\n",
    "    else:\n",
    "        print(\"Usage: python tuning-params.py <automation_dir> <attpcroot_dir> <iteration>\")\n",
    "        raise ValueError(\"Incorrect number of arguments passed to tuning-params.py\")\n",
    "rseed += iteration # to ensure that the random seed is different for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_file(file_type, indicator_directory=automation_dir):\n",
    "    df = pd.DataFrame([0])\n",
    "    df.to_csv(indicator_directory + file_type + '.csv', index=False)\n",
    "    print(file_type + ' FILE CREATED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_to_momentum(energy, particle):\n",
    "    # input energy in KeV, convert to MeV\n",
    "    energy = energy/1000\n",
    "\n",
    "    # Mass values from NIST\n",
    "    if particle == 'a':\n",
    "        mass = 3727.3794066 # MeV/c^2\n",
    "    elif particle == 'p':\n",
    "        mass = 938.27208816 # MeV/c^2\n",
    "    else:\n",
    "        indicator_file('STOP')\n",
    "        raise Exception('Error: particle must be \"a\" or \"p\"')\n",
    "    momentum = np.sqrt(2*mass*energy)/1000 # GeV/c\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_H5(automation_dir, file, threshold, ADD_NOISE=False):\n",
    "    def pca_func(data, n_components):\n",
    "        \n",
    "        # original method\n",
    "        #pca = PCA(n_components=n_components)\n",
    "        #pca.fit(data)\n",
    "        #out = pca.transform(data)\n",
    "        \n",
    "        # fishtank-compatible method\n",
    "        # Subtract the mean from the data to center it\n",
    "        mean = np.mean(data, axis=0)\n",
    "        data_centered = data - mean\n",
    "        \n",
    "        # Compute the covariance matrix\n",
    "        cov = np.cov(data_centered, rowvar=False)\n",
    "\n",
    "        # Compute the eigenvectors and eigenvalues of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eig(cov)\n",
    "\n",
    "        # Transform the data into the new coordinate system defined by the eigenvectors\n",
    "        out = np.dot(data_centered, eigvecs)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def dbscan(X, eps, min_samples):\n",
    "        # Initialize variables\n",
    "        labels = np.zeros(X.shape[0])\n",
    "        cluster = 0\n",
    "\n",
    "        # Compute distances between points\n",
    "        dists = np.sqrt(((X[:, np.newaxis] - X) ** 2).sum(axis=2))\n",
    "\n",
    "        # Iterate over each point\n",
    "        for i in range(X.shape[0]):\n",
    "            # If point already visited, continue\n",
    "            if labels[i] != 0:\n",
    "                continue\n",
    "\n",
    "            # Find neighboring points\n",
    "            neighbors = np.where(dists[i] <= eps)[0]\n",
    "\n",
    "            # If not enough neighboring points, label as noise\n",
    "            if len(neighbors) < min_samples:\n",
    "                labels[i] = -1\n",
    "                continue\n",
    "\n",
    "            # Expand cluster\n",
    "            cluster += 1\n",
    "            labels[i] = cluster\n",
    "\n",
    "            while len(neighbors) > 0:\n",
    "                j = neighbors[0]\n",
    "                if labels[j] == -1:\n",
    "                    labels[j] = cluster\n",
    "                elif labels[j] == 0:\n",
    "                    labels[j] = cluster\n",
    "                    new_neighbors = np.where(dists[j] <= eps)[0]\n",
    "                    if len(new_neighbors) >= min_samples:\n",
    "                        neighbors = np.concatenate((neighbors, new_neighbors))\n",
    "                neighbors = neighbors[1:]\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def remove_outliers(xset, yset, zset, eset, threshold):\n",
    "        #Uses DBSCAN to find and remove outliers in 3D data\n",
    "        # NEEDS ALTERNATE METHOD FOR FISHTANK COMPATIBILITY\n",
    "        data = np.array([xset.T, yset.T, zset.T]).T\n",
    "\n",
    "        # STANDARD METHOD\n",
    "        #DBSCAN_cluster = DBSCAN(eps=7, min_samples=10).fit(data)\n",
    "        #out_of_cluster_index = np.where(DBSCAN_cluster.labels_==-1)\n",
    "\n",
    "        # FISHTANK METHOD\n",
    "        labels = dbscan(data, eps=7, min_samples=10)\n",
    "        out_of_cluster_index = np.where(labels==-1)\n",
    "\n",
    "        del data\n",
    "        rev = out_of_cluster_index[0][::-1]\n",
    "\n",
    "        for i in rev:\n",
    "            xset = np.delete(xset, i)\n",
    "            yset = np.delete(yset, i)\n",
    "            zset = np.delete(zset, i)\n",
    "            eset = np.delete(eset, i)\n",
    "        if len(xset) <= threshold:\n",
    "            veto = True\n",
    "        else:\n",
    "            veto = False\n",
    "\n",
    "        # testing\n",
    "        veto = False\n",
    "        \n",
    "        return xset, yset, zset, eset, veto\n",
    "    \n",
    "    def track_len(xset, yset, zset):\n",
    "        \"\"\"\n",
    "        Uses PCA to find the length of a track\n",
    "        \"\"\"\n",
    "        veto_on_length = False\n",
    "    \n",
    "        # Form data matrix\n",
    "        data = np.concatenate((xset[:, np.newaxis], \n",
    "                               yset[:, np.newaxis], \n",
    "                               zset[:, np.newaxis]), \n",
    "                               axis=1)\n",
    "    \n",
    "        # Use PCA to find track length\n",
    "        principalComponents = pca_func(data, 3)\n",
    "    \n",
    "        principalDf = pd.DataFrame(data = principalComponents\n",
    "                 , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "        \n",
    "        track_len = 2.35*principalDf.std()[0]\n",
    "        track_width = 2.35*principalDf.std()[1]\n",
    "        track_depth = 2.35*principalDf.std()[2]\n",
    "        #if track_len > 70:\n",
    "        #    veto_on_length = True\n",
    "        \n",
    "        return track_len, veto_on_length, track_width, track_depth\n",
    "    \n",
    "    def main(h5file, threshold, ADD_NOISE):\n",
    "        \"\"\"\n",
    "        This functions does the following: \n",
    "        - Converts h5 files into ndarrays. \n",
    "        - Removes outliers.\n",
    "        - Calls PCA to return track length.\n",
    "        - Sums mesh signal to return energy.\n",
    "        \"\"\"\n",
    "        # Converts h5 files into ndarrays, and output each event dataset as a separte list\n",
    "        num_events = int(len(list(h5file.keys()))/2) \n",
    "        \n",
    "        good_events = []\n",
    "        \n",
    "        length_list = []\n",
    "        width_list = []\n",
    "        depth_list = []\n",
    "        \n",
    "        tot_energy = []\n",
    "        tracemax_list = []\n",
    "        peakw_list = []\n",
    "        \n",
    "        maxe_list = []\n",
    "        stde_list = []\n",
    "        padnum_list = []\n",
    "        \n",
    "        skipped_events = 0\n",
    "        veto_events = 0\n",
    "        \n",
    "        #pbar = tqdm(total=num_events+1)\n",
    "        for i in range(0, num_events):\n",
    "            str_event = f\"Event_[{i}]\"\n",
    "            \n",
    "            # Apply pad threshold\n",
    "            event = np.array(h5file[str_event][:])\n",
    "            if len(event) <= threshold:\n",
    "                skipped_events += 1\n",
    "                #pbar.update(n=1)\n",
    "                continue\n",
    "                \n",
    "            # Make copy of datasets\n",
    "            dset_0_copyx = event['x']\n",
    "            dset_0_copyy = event['y'] \n",
    "            dset_0_copyz = event['z'] - min(event['z'])\n",
    "            dset_0_copye = event['A']\n",
    "            \n",
    "            # Apply veto condition\n",
    "            R = 36                           # Radius of the pad plane\n",
    "            r = np.sqrt(dset_0_copyx**2 + dset_0_copyy**2)\n",
    "            statements = np.greater(r, R)    # Check if any point lies outside of R\n",
    "        \n",
    "            if np.any(statements) == True:\n",
    "                veto_events += 1\n",
    "                #pbar.update(n=1)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # Call remove_outliers to get dataset w/ outliers removed\n",
    "            dset_0_copyx, dset_0_copyy, dset_0_copyz, dset_0_copye, veto = remove_outliers(dset_0_copyx, dset_0_copyy, dset_0_copyz, dset_0_copye, threshold)\n",
    "            veto = False\n",
    "            if veto == True:\n",
    "                skipped_events += 1\n",
    "                #pbar.update(n=1)\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # Call track_len() to create lists of all track lengths\n",
    "            length, veto_on_length, width, depth = track_len(dset_0_copyx, dset_0_copyy, dset_0_copyz)\n",
    "            if veto_on_length == True:\n",
    "                veto_events += 1\n",
    "                #pbar.update(n=1)\n",
    "                continue\n",
    "\n",
    "            \n",
    "            str_trace = f\"Trace_[{i}]\"\n",
    "            trace = np.array(h5file[str_trace][:])\n",
    "            max_val = np.argmax(trace)\n",
    "            low_bound = max_val - 75\n",
    "            if low_bound < 0:\n",
    "                low_bound = 5\n",
    "            upper_bound = max_val + 75\n",
    "            if upper_bound > 511:\n",
    "                upper_bound = 506\n",
    "            trace = trace[low_bound:upper_bound]\n",
    "\n",
    "            # STANDARD METHOD\n",
    "            #baseObj=BaselineRemoval(trace)\n",
    "            #trace=baseObj.IModPoly(polynomial_degree)\n",
    "\n",
    "            # FISHTANK METHOD\n",
    "            # determine the width of the peak in the trace and the location of the peak\n",
    "            peakloc = np.argmax(trace)\n",
    "            \n",
    "            peakwidth1 = 0\n",
    "            peakwidth2 = 0\n",
    "\n",
    "            k = peakloc\n",
    "            while trace[k] > np.min(trace) + np.std(trace):\n",
    "                peakwidth1 += 1\n",
    "                k += 1\n",
    "            k = peakloc\n",
    "            while trace[k] > np.min(trace) + np.std(trace):\n",
    "                peakwidth2 += 1\n",
    "                k -= 1\n",
    "\n",
    "            # calculate the average of the trace outside of the peakwidth window on either side of the peak\n",
    "            baseline = np.mean(np.concatenate((trace[:peakloc-peakwidth2], trace[peakloc+peakwidth1:])))\n",
    "            # subtract the baseline from the trace\n",
    "            trace = trace - baseline\n",
    "\n",
    "            # ADD NOISE TO TRACE\n",
    "            # noise of frequency ~0.25 with random phase\n",
    "            if ADD_NOISE:\n",
    "                noise_peak = np.sin(np.linspace(0, 2*np.pi*len(trace)*random.gauss(2.49815e-1, -8.27021e-3), len(trace)) + random.random()*2*np.pi)\n",
    "\n",
    "                noise_background = np.zeros_like(trace)\n",
    "                for freq in np.linspace(0.1, 0.5):\n",
    "                    noise_background += np.sin(np.linspace(0, 2*np.pi*len(trace)*freq, len(trace)) + random.random()*2*np.pi) * random.gauss(1, 0.3)\n",
    "                noise_background = noise_background - np.mean(noise_background)\n",
    "                noise_background = noise_background / np.std(noise_background)\n",
    "\n",
    "\n",
    "                noise = 13000*noise_peak + 2000 * noise_background\n",
    "\n",
    "                # normalize noise to std of 1 and average of 0, then multiply by amplitude of real data (402.57 +- 106.40 std)\n",
    "                noise = (noise - np.mean(noise))/np.std(noise) * random.gauss(402.57, 106.40)\n",
    "                # add noise to trace\n",
    "                trace = trace + noise\n",
    "            \n",
    "            # VETO ON TRACE SUM\n",
    "            #if np.sum(trace) > 800000:\n",
    "            #    veto_events += 1\n",
    "            #    pbar.update(n=1)\n",
    "            #    continue\n",
    "\n",
    "            length_list.append(length)\n",
    "            width_list.append(width)\n",
    "            depth_list.append(depth)\n",
    "            \n",
    "            tot_energy.append(np.sum(trace))\n",
    "            tracemax_list.append(np.max(trace))\n",
    "            peakw_list.append(peakwidth1 + peakwidth2)\n",
    "            \n",
    "            maxe_list.append(np.max(dset_0_copye))\n",
    "            stde_list.append(np.std(dset_0_copye))\n",
    "            padnum_list.append(len(dset_0_copyx))\n",
    "\n",
    "            # Track event number of good events\n",
    "            good_events.append(i)  \n",
    "            #pbar.update(n=1)\n",
    "            \n",
    "        # package lists into a dictionary\n",
    "        output_dict = {\n",
    "            \"length\": length_list,\n",
    "            \"width\": width_list,\n",
    "            \"depth\": depth_list,\n",
    "            \"trace_sum\": tot_energy,\n",
    "            \"trace_max\": tracemax_list,\n",
    "            \"peak_width\": peakw_list,\n",
    "            \"max_e\": maxe_list,\n",
    "            \"std_e\": stde_list,\n",
    "            \"padnum\": padnum_list}\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    h5f = h5py.File(automation_dir + 'simOutput/' + file, 'r')\n",
    "    \n",
    "    output_dict = main(h5file=h5f, threshold=5, ADD_NOISE=ADD_NOISE)\n",
    "    \n",
    "    results_df = pd.DataFrame(columns=['file', 'ptype', 'event#',\n",
    "                                       'length', 'width', 'depth',\n",
    "                                       'trace_sum', 'trace_max', 'peak_width',\n",
    "                                       'max_e', 'std_e', 'padnum'])\n",
    "    \n",
    "    file_name = file.split('.h5')[0]\n",
    "    ptype = file_name.split('-')[-1]\n",
    "    \n",
    "    for i in range(len(output_dict['length'])):\n",
    "        results_df.loc[i] = [file_name, ptype, i,\n",
    "                             output_dict['length'][i], output_dict['width'][i], output_dict['depth'][i],\n",
    "                             output_dict['trace_sum'][i], output_dict['trace_max'][i], output_dict['peak_width'][i],\n",
    "                             output_dict['max_e'][i], output_dict['std_e'][i], output_dict['padnum'][i]]\n",
    "        \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first iteration setup\n",
    "if iteration == 0:\n",
    "    print('First iteration, setting up files')\n",
    "    \n",
    "    # create tuning log file\n",
    "    tuning_log = pd.DataFrame(columns=['file', 'ptype', 'event#',\n",
    "                                       'length', 'width', 'depth',\n",
    "                                       'trace_sum', 'trace_max', 'peak_width',\n",
    "                                       'max_e', 'std_e', 'padnum'])\n",
    "    \n",
    "    # READ IN REFERENCE FILES AND ANALYZE FOR COMPARISON\n",
    "    reference_files = []\n",
    "    # look for reference h5 files in simOutput\n",
    "    for file in os.listdir(automation_dir + 'simOutput/'):\n",
    "        if file.endswith('.h5') and file.startswith('ref-'):\n",
    "            reference_files.append(file)\n",
    "    \n",
    "    # analyze reference files\n",
    "    for file in reference_files:\n",
    "        print('Analyzing reference file: ' + file)\n",
    "        results = Analyze_H5(automation_dir, file, 1, ADD_NOISE=False)\n",
    "        tuning_log = tuning_log.append(results, ignore_index=True)\n",
    "    \n",
    "    # write tuning log to csv\n",
    "    tuning_log.to_csv(automation_dir + 'simOutput/tuning_log.csv', index=False)\n",
    "    \n",
    "    # SETUP OF FIRST ITERATION SIMULATION\n",
    "    # initialize tuning parameters\n",
    "    param_df = pd.DataFrame(columns=['Sim', 'Status', 'N', 'P0', 'E0', 'P1', 'E1', 'Xb', 'Yb', 'Zb1', 'Zb2', 'Threshold'])\n",
    "    \n",
    "    # prompt user for list of variables to tune, and initial values and variation\n",
    "    tuning_params = {}\n",
    "    print('Enter parameters to tune, along with initial value and percent bounding range (Separate by spaces)')\n",
    "    print('Example: N 1000 10') # N variable, centered at 1000 with 10% bounds\n",
    "    print('Input a blank line to finish')\n",
    "    while True:\n",
    "        user_input = input()\n",
    "        if user_input == '':\n",
    "            if len(tuning_params) == 0:\n",
    "                print('No tuning parameters entered, please enter at least one')\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            split_input = user_input.split()\n",
    "            if len(split_input) != 3:\n",
    "                print('Invalid input, please try again')\n",
    "            else:\n",
    "                param, val, var = split_input\n",
    "                if param in tuning_params or param in ['Sim', 'Status', 'P0', 'E0', 'P1', 'E1', 'Xb', 'Yb', 'Zb1', 'Zb2']:\n",
    "                    print('Parameter specification invalid, please try again')\n",
    "                else:\n",
    "                    try :\n",
    "                        val = float(val)\n",
    "                        var = float(var)\n",
    "                        tuning_params[param] = [float(val), val - (var * val / 100), val + (var * val / 100)] # store as [center, -bound, +bound]\n",
    "                    except ValueError:\n",
    "                        print('Invalid input, please try again')\n",
    "    \n",
    "    # write initial tuning parameters to param_df\n",
    "    for ptype in tuning_log['ptype'].unique():\n",
    "        # get p0, e0, p1, e1 from reference files\n",
    "        ptypel = ptype.split(' ')\n",
    "        p0, e0 = ptypel[0][-1], ptypel[0][:-1]\n",
    "        print(p0, e0)\n",
    "        if len(ptypel) == 2:\n",
    "            p1, e1 = ptypel[1][-1], ptypel[1][:-1]\n",
    "            print(p1, e1)\n",
    "        else:\n",
    "            p1, e1 = 'a', 0\n",
    "        \n",
    "        # name simulation(s)\n",
    "        simname = 'tuning' + str(iteration) + '-' + ptype\n",
    "        \n",
    "        if len(param_df) == 0:\n",
    "            param_df.loc[0] = [simname, 0, 100, p0, e0, p1, e1, 0, 0, 0, 0, 1]\n",
    "        else:\n",
    "            # copy previous row\n",
    "            param_df.loc[len(param_df)] = param_df.loc[len(param_df)-1].copy()\n",
    "        \n",
    "        # write values to param_df\n",
    "        param_df.loc[len(param_df)-1, 'Sim'] = simname\n",
    "        param_df.loc[len(param_df)-1, 'P0'] = p0; param_df.loc[len(param_df)-1, 'E0'] = e0\n",
    "        param_df.loc[len(param_df)-1, 'P1'] = p1; param_df.loc[len(param_df)-1, 'E1'] = e1\n",
    "        \n",
    "        # add columns to param_df for tuning parameters\n",
    "        for param in tuning_params:\n",
    "            # get initial value\n",
    "            val = tuning_params[param][0]\n",
    "            param_df.loc[len(param_df)-1, param] = val\n",
    "            \n",
    "    # add Score column to param_df\n",
    "    param_df['Score'] = -1 # -1 indicates not yet scored\n",
    "        \n",
    "    # write param_df to csv\n",
    "    param_df.to_csv(automation_dir + 'simInput/parameters.csv', index=False)\n",
    "    \n",
    "    # prompt user for DecayRate, Target, MaxIterations, BestN\n",
    "    \n",
    "    SavedSettings = {}\n",
    "    print('Enter tuning parameters')\n",
    "    SavedSettings['BaseVar'] = float(input('Base Variation: '))\n",
    "    SavedSettings['DecayRate'] = float(input('DecayRate: '))\n",
    "    SavedSettings['Target'] = float(input('Target: '))\n",
    "    SavedSettings['MaxIterations'] = float(input('MaxIterations: '))\n",
    "    SavedSettings['BestN'] = int(input('BestN: '))\n",
    "    \n",
    "    SavedSettings['ptypes'] = tuning_log['ptype'].unique().tolist()\n",
    "    tuning_params['Settings'] = SavedSettings\n",
    "        \n",
    "    # save intial tuning parameters to json file\n",
    "    with open(automation_dir + 'simOutput/tuning_params.json', 'w') as f:\n",
    "        json.dump(tuning_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_analysis(tuning_log, refname, simname):\n",
    "    # compare analysis of reference file to simulation file\n",
    "    ref_data = tuning_log[tuning_log['file'] == refname]\n",
    "    sim_data = tuning_log[tuning_log['file'] == simname]\n",
    "    \n",
    "    score = 0\n",
    "    cols = 0\n",
    "    \n",
    "    sampled_cols = ['Length', 'Trace_sum'] # columns that are selected for, standard deviation should not be compared\n",
    "    \n",
    "    for col in ref_data.columns:\n",
    "        if col in ['file', 'ptype', 'event#']:\n",
    "            continue\n",
    "        else:\n",
    "            cols += 1\n",
    "            #score += stats.ks_2samp(ref_data[col], sim_data[col])[0]\n",
    "            score += np.abs(np.mean(ref_data[col]) - np.mean(sim_data[col])) / np.mean(ref_data[col]) # compare means\n",
    "            #if col not in sampled_cols:\n",
    "            #    score += np.abs(np.std(ref_data[col]) - np.std(sim_data[col])) / np.std(ref_data[col]) # compare stds\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file: tuning0-800p.h5\n",
      "Analyzing file: tuning0-500a.h5\n"
     ]
    }
   ],
   "source": [
    "# subsequent iterations\n",
    "# load in parameters.csv\n",
    "param_df = pd.read_csv(automation_dir + 'simInput/parameters.csv')\n",
    "\n",
    "# test if all queued simulations are complete\n",
    "if iteration > 0 and len(param_df[param_df['Status'] == 0]) == 0:\n",
    "    if param_df[param_df['Status'] == 1].shape[0] > 0:\n",
    "        # temporarily rename unnamed output.h5 to corresponding simulation name\n",
    "        for file in os.listdir(automation_dir + 'simOutput/'):\n",
    "            if file == 'output.h5':\n",
    "                new_name = param_df.loc[param_df['Status'] == 1, 'Sim'].values[0] + '.h5'\n",
    "                os.rename(automation_dir + 'simOutput/output.h5', automation_dir + 'simOutput/' + new_name)        \n",
    "    \n",
    "    # analyze all simulations not yet analyzed\n",
    "    tuning_log = pd.read_csv(automation_dir + 'simOutput/tuning_log.csv')\n",
    "    for file in os.listdir(automation_dir + 'simOutput/'):\n",
    "        if file.endswith('.h5') and file[:-3] not in tuning_log['file'].values:\n",
    "            print('Analyzing file: ' + file)\n",
    "            results = Analyze_H5(automation_dir, file, 1, ADD_NOISE=False)\n",
    "            tuning_log = tuning_log.append(results, ignore_index=True)\n",
    "    \n",
    "    # calculate score for each simulation and write to param_df\n",
    "    for i in range(len(param_df)):\n",
    "        if param_df.loc[i, 'Score'] == -1: # if score has not been calculated\n",
    "            refname = 'ref-' + str(param_df.loc[i, 'E0']) + param_df.loc[i, 'P0'] \n",
    "            if param_df.loc[i, 'P1'] != 'a' and param_df.loc[i, 'E1'] != 0:\n",
    "                refname += ' ' + str(param_df.loc[i, 'E1']) + param_df.loc[i, 'P1']\n",
    "            simname = param_df.loc[i, 'Sim']\n",
    "            \n",
    "            score = compare_analysis(tuning_log, refname, simname)\n",
    "            param_df.loc[i, 'Score'] = score\n",
    "    \n",
    "     \n",
    "    with open(automation_dir + 'simOutput/tuning_params.json', 'r') as f:\n",
    "        tuning_params = json.load(f)\n",
    "    \n",
    "    # test for end of tuning\n",
    "    if iteration >= tuning_params['Settings']['MaxIterations']:\n",
    "        print('Tuning Iterations exceeded maximum, ending tuning')\n",
    "        sys.exit()\n",
    "    elif min(param_df['Score']) <= tuning_params['Settings']['Target']:\n",
    "        print('Target score reached, ending tuning')\n",
    "        sys.exit()\n",
    "    \n",
    "    best_param_df = 0\n",
    "    \n",
    "    # sort param_df by score for each particle type\n",
    "    for p in tuning_params['Settings']['ptypes']:\n",
    "        if best_param_df == 0:\n",
    "            best_param_df = param_df[(param_df['Sim'].str.split('-').str[1].eq(p))].sort_values(by=['Score']).head(tuning_params['Settings']['BestN'])\n",
    "        else:\n",
    "            best_param_df = best_param_df.append(param_df[(param_df['Sim'].str.split('-').str[1].eq(p))].sort_values(by=['Score']).head(tuning_params['Settings']['BestN']))\n",
    "    \n",
    "    # determine new central values for each parameter\n",
    "    for param in tuning_params:\n",
    "        if param == 'Settings':\n",
    "            continue\n",
    "        else:\n",
    "            old_val = tuning_params[param][0] # save old value for comparison\n",
    "            tuning_params[param][0] = best_param_df[param].mean() # set new central value to mean of best simulations\n",
    "            #if tuning_params[param][0] == old_val:\n",
    "            #    random.seed(rseed - 1) # better value found, use previous random seed for optimization momentum\n",
    "    \n",
    "    # determine current decay of variation\n",
    "    decay = math.exp(-iteration/tuning_params['Settings']['DecayRate']*math.log(2))\n",
    "    \n",
    "    # generate new parameter values\n",
    "    queued_params = {}\n",
    "    for param in tuning_params:\n",
    "        if param == 'Settings':\n",
    "            continue\n",
    "        else:\n",
    "            var = tuning_params[param][0] * tuning_params['Settings']['BaseVar'] * decay / 100\n",
    "            val0 = tuning_params[param][0]\n",
    "            queued_params[param] = np.clip(np.random.uniform(val0 - var, val0 + var), tuning_params[param][1], tuning_params[param][2]) # generate new parameter value within bounds\n",
    "            \n",
    "            # basic error checking to ensure parameter is within bounds and correct type (int or float)\n",
    "            if param in ['N', 'PadPlaneX', 'PadPlaneZ', 'PadSizeX', 'PadSizeZ', 'PadRows', 'PadLayers', 'NumTbs', 'YDivider', 'GasFile', 'PadPlaneFile', 'PadShapeFile', 'TB0', 'TBEntrance', 'PeakingTime']:\n",
    "                queued_params[param] = int(queued_params[param])\n",
    "            if queued_params[param] * tuning_params[param][0] <= 0:\n",
    "                queued_params[param] = val0\n",
    "                \n",
    "    \n",
    "    # create new simulations based on new parameter values\n",
    "    for p in tuning_params['Settings']['ptypes']:\n",
    "        param_df.loc[len(param_df)] = param_df[(param_df['Sim'].str.split('-').str[1].eq(p))].sort_values(by=['Score']).head(1).iloc[0]\n",
    "        param_df.loc[len(param_df)-1, 'Sim'] = 'tuning' + str(iteration) + '-' + p\n",
    "        param_df.loc[len(param_df)-1, 'Status'] = 0\n",
    "        param_df.loc[len(param_df)-1, 'Score'] = -1\n",
    "        for param in queued_params:\n",
    "            param_df.loc[len(param_df)-1, param] = queued_params[param]\n",
    "\n",
    "    # write param_df to csv\n",
    "    param_df.to_csv(automation_dir + 'simInput/parameters.csv', index=False)\n",
    "    \n",
    "    # write tuning_params to json\n",
    "    with open(automation_dir + 'simOutput/tuning_params.json', 'w') as f:\n",
    "        json.dump(tuning_params, f, indent=4)\n",
    "    \n",
    "    # write tuning_log to csv\n",
    "    tuning_log.to_csv(automation_dir + 'simOutput/tuning_log.csv', index=False)\n",
    "    \n",
    "    # reset naming of latest output.h5\n",
    "    os.rename(automation_dir + 'simOutput/' + new_name, automation_dir + 'simOutput/output.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
